COURSE PROJECT: PRACTICAL MACHINE LEARNING 
Data Cleaning and Preparation

train_in <- read.csv('./pml-training.csv', header=T)
validation <- read.csv('./pml-testing.csv', header=T)
Data Partitioning
set.seed(127)
training_sample <- createDataPartition(y=train_in$classe, p=0.7, list=FALSE)
training <- train_in[training_sample, ]
testing <- train_in[-training_sample, ]
Identification on Non-Zero Data
Remove NearZeroVariance variables
all_zero_colnames <- sapply(names(validation), function(x) all(is.na(validation[,x])==TRUE))
nznames <- names(all_zero_colnames)[all_zero_colnames==FALSE]
nznames <- nznames[-(1:7)]
nznames <- nznames[1:(length(nznames)-1)]
The models will be fit using the following data columns:
  ##  [1] "accel_arm_x"          "accel_arm_y"          "accel_arm_z"         
  ##  [4] "accel_belt_x"         "accel_belt_y"         "accel_belt_z"        
  ##  [7] "accel_dumbbell_x"     "accel_dumbbell_y"     "accel_dumbbell_z"    
  ## [10] "accel_forearm_x"      "accel_forearm_y"      "accel_forearm_z"     
  ## [13] "gyros_arm_x"          "gyros_arm_y"          "gyros_arm_z"         
  ## [16] "gyros_belt_x"         "gyros_belt_y"         "gyros_belt_z"        
  ## [19] "gyros_dumbbell_x"     "gyros_dumbbell_y"     "gyros_dumbbell_z"    
  ## [22] "gyros_forearm_x"      "gyros_forearm_y"      "gyros_forearm_z"     
  ## [25] "magnet_arm_x"         "magnet_arm_y"         "magnet_arm_z"        
  ## [28] "magnet_belt_x"        "magnet_belt_y"        "magnet_belt_z"       
  ## [31] "magnet_dumbbell_x"    "magnet_dumbbell_y"    "magnet_dumbbell_z"   
## [34] "magnet_forearm_x"     "magnet_forearm_y"     "magnet_forearm_z"    
## [37] "pitch_arm"            "pitch_belt"           "pitch_dumbbell"      
## [40] "pitch_forearm"        "roll_arm"             "roll_belt"           
## [43] "roll_dumbbell"        "roll_forearm"         "total_accel_arm"     
## [46] "total_accel_belt"     "total_accel_dumbbell" "total_accel_forearm" 
## [49] "yaw_arm"              "yaw_belt"             "yaw_dumbbell"        
## [52] "yaw_forearm"
Model building
The three model types used are:
  1.  Decision trees with CART (rpart)
2.	Stochastic gradient boosting trees (gbm)
3.	Random forest decision trees (rf)
model_cart <- train(
  classe ~ ., 
  data=training[, c('classe', nznames)],
  trControl=fitControl,
  method='rpart'
)
save(model_cart, file='./ModelFitCART.RData')
model_gbm <- train(
  classe ~ ., 
  data=training[, c('classe', nznames)],
  trControl=fitControl,
  method='gbm'
)
save(model_gbm, file='./ModelFitGBM.RData')
model_rf <- train(
  classe ~ ., 
  data=training[, c('classe', nznames)],
  trControl=fitControl,
  method='rf',
  ntree=100
)
save(model_rf, file='./ModelFitRF.RData')

Cross validation
Cross validation is done for each model with K = 3. 
fitControl <- trainControl(method='cv', number = 3)
Model Assessment (Out of sample error)
predCART <- predict(model_cart, newdata=testing)
cmCART <- confusionMatrix(predCART, testing$classe)
predGBM <- predict(model_gbm, newdata=testing)
cmGBM <- confusionMatrix(predGBM, testing$classe)
predRF <- predict(model_rf, newdata=testing)
cmRF <- confusionMatrix(predRF, testing$classe)
AccuracyResults <- data.frame(
  Model = c('CART', 'GBM', 'RF'),
  Accuracy = rbind(cmCART$overall[1], cmGBM$overall[1], cmRF$overall[1])
)
print(AccuracyResults)
##   Model  Accuracy
## 1  CART 0.4932880
## 2   GBM 0.9622770
## 3    RF 0.9926933
Based on an assessment of these 3 model fits and out-of-sample results, it looks like both gradient boosting and random forests outperform the CART model, with random forests being slightly more accurate. The confusion matrix for the random forest model is below.
##           Reference
## Prediction    A    B    C    D    E
##          A 1671    9    0    0    0
##          B    3 1126    4    4    2
##          C    0    4 1020    6    1
##          D    0    0    2  952    6
##          E    0    0    0    2 1073
